{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abschlussprojekt - Diskriminierung in Machine Learning Modellen\n",
    "\n",
    "Dieses Notebook analysiert den `adult.csv`-Datensatz unter besonderer Berücksichtigung von Fairness-Aspekten in Machine-Learning-Modellen.\n",
    "\n",
    "### Zielsetzung:\n",
    "- **Explorative Datenanalyse (EDA):** Siehe EDA-Notebook\n",
    "- **Machine Learning Modelle:** Vergleich mehrerer Modelle hinsichtlich ihrer Vorhersagegenauigkeit und Fairness.\n",
    "- **Fairness-Analyse:** Berechnung von Fairness-Metriken zur Bewertung der Diskriminierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import notwendiger Bibliotheken\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from data_cleaning import fill_missing_values, rename_columns\n",
    "from data_science_skript import preprocess_data, generate_plots, train_and_predict, evaluate_model, evaluate_discrimination\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "import optuna\n",
    "import time\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "\n",
    "from fairlearn.metrics import equalized_odds_difference, demographic_parity_difference, demographic_parity_ratio\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "\n",
    "# Sytle für Plots\n",
    "plt.style.use(\"dark_background\")\n",
    "colors = [\"silver\", \"teal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden und aufbereiten der Daten\n",
    "df = pd.read_csv(r\"C:\\Users\\kimko\\PortfolioProjekt\\adult.csv\", na_values=[\"?\"]) \n",
    "df = fill_missing_values(df) \n",
    "df = rename_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knappe Explorative Datenanalyse (EDA) hinsichtlich der Einkommensverteilung nach Geschlecht und Ethnie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Einkommensverteilung nach Geschlecht\n",
    "df.groupby([\"sex\", \"income\"]).size().unstack().plot(kind=\"bar\", stacked=True, color=colors, ax=ax[0])\n",
    "ax[0].set_title(\"Einkommensverteilung nach Geschlecht\")\n",
    "ax[0].set_xlabel(\"Geschlecht\")\n",
    "ax[0].set_ylabel(\"Anzahl\")\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=0)\n",
    "\n",
    "# Einkommensverteilung nach Ethnie\n",
    "df.groupby([\"race\", \"income\"]).size().unstack().plot(kind=\"bar\", stacked=True, color=colors, ax=ax[1])\n",
    "ax[1].set_title(\"Einkommensverteilung nach Ethnie\")\n",
    "ax[1].set_xlabel(\"Ethnie\")\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variablen umwandeln\n",
    "df = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Modelle\n",
    "\n",
    "- Logistische Regression\n",
    "- Decision Tree\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vorbereitung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test-Split\n",
    "pickle_train_test = \"data_science/train_test_split.pkl\"\n",
    "\n",
    "if os.path.exists(pickle_train_test):\n",
    "    print(\"Lade bereits gespeicherte Trainings- und Testdaten...\")\n",
    "    with open(pickle_train_test, \"rb\") as file:\n",
    "        features_train, features_test, target_train, target_test = pickle.load(file)\n",
    "else:\n",
    "    print(\"Erstelle neue Trainings- und Testdaten...\")\n",
    "    \n",
    "    # Daten splitten\n",
    "    target = df[\"income\"]\n",
    "    features = df.drop(columns=[\"income\"])\n",
    "\n",
    "    features_train, features_test, target_train, target_test = train_test_split(\n",
    "        features, target, test_size=0.2, stratify=target, random_state=42\n",
    "    )\n",
    "\n",
    "    # Daten speichern\n",
    "    with open(pickle_train_test, \"wb\") as file:\n",
    "        pickle.dump((features_train, features_test, target_train, target_test), file)\n",
    "\n",
    "    print(\"Daten wurden gespeichert!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Trainingsdaten:\\n\",features_train.shape)\n",
    "print(\"\\nTestdaten:\\n\",features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.countplot(x=target_train, ax=ax[0])\n",
    "ax[0].set_title(\"Klassenverteilung im Training-Set\")\n",
    "\n",
    "sns.countplot(x=target_test, ax=ax[1])\n",
    "ax[1].set_title(\"Klassenverteilung im Test-Set\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_crosstab_income = pd.crosstab(index=target_train, columns = \"count\", normalize = \"columns\")\n",
    "test_crosstab_income = pd.crosstab(index=target_test, columns = \"count\", normalize = \"columns\")\n",
    "display(train_crosstab_income)\n",
    "display(test_crosstab_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistische Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorbereitung\n",
    "num_cols = features_train.select_dtypes(include=[\"int64\"]).columns\n",
    "cat_cols = features_train.select_dtypes(include=[\"object\"]).columns \n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    (\"num\", StandardScaler(), num_cols),\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "])\n",
    "\n",
    "# Baselinemodell Logistische Regression\n",
    "pipeline_log_base = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", LogisticRegression(class_weight=\"balanced\", random_state = 42))\n",
    "     ])\n",
    "\n",
    "target_pred_log = train_and_predict(pipeline_log_base, features_train, target_train, features_test, target_test)\n",
    "metrics_log = evaluate_model(target_test, target_pred_log)\n",
    "metrics_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "pipeline_dt = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", DecisionTreeClassifier(class_weight=\"balanced\", random_state = 42))\n",
    "     ])\n",
    "\n",
    "target_pred_dt = train_and_predict(pipeline_dt, features_train, target_train, features_test, target_test)\n",
    "metrics_dt = evaluate_model(target_test, target_pred_dt)\n",
    "metrics_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "target_pred_rf = train_and_predict(pipeline_rf, features_train, target_train, features_test, target_test)\n",
    "metrics_rf = evaluate_model(target_test, target_pred_rf)\n",
    "metrics_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse der drei Basismodelle\n",
    "model_results = {\n",
    "    \"Logistic Regression\": metrics_log,\n",
    "    \"Decision Tree\": metrics_dt,\n",
    "    \"Random Forest\": metrics_rf\n",
    "}\n",
    "\n",
    "# Extrahieren der Metriken\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "model_names = list(model_results.keys())\n",
    "\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(metrics))\n",
    "color_model= [\"silver\", \"teal\", \"midnightblue\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (model, color) in enumerate(zip(model_names, color_model)):\n",
    "    values = [model_results[model][metric] for metric in metrics]\n",
    "    ax.bar(x + i * bar_width, values, width=bar_width, label=model, color=color)\n",
    "\n",
    "ax.set_xticks(x + bar_width)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Modellvergleich der Metriken\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Importance des Base-Line Modells\n",
    "feature_names = list(num_cols) + list(pipeline_log_base.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "coefficients = pipeline_log_base.named_steps[\"model\"].coef_[0]\n",
    "feature_importance_log = pd.Series(data = pipeline_log_base.named_steps[\"model\"].coef_[0],\n",
    "                               index = feature_names).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Feature Importance für Baummodelle\n",
    "feature_names = list(num_cols) + list(pipeline_rf.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(cat_cols))\n",
    "\n",
    "# Für den DecisionTree\n",
    "feature_importance_dt = pd.Series(data=pipeline_dt.named_steps[\"model\"].feature_importances_,\n",
    "                               index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "#Für den RandomForest\n",
    "feature_importance_rf = pd.Series(data=pipeline_rf.named_steps[\"model\"].feature_importances_,\n",
    "                               index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8, 12)) \n",
    "feature_importance_log.head(20).plot(kind='barh', ax = ax[0])\n",
    "feature_importance_dt.head(20).plot(kind='barh', ax = ax[1])\n",
    "feature_importance_rf.head(20).plot(kind='barh', ax = ax[2])\n",
    "\n",
    "ax[0].set_title(\"Feature Importance Logistische Regression\")\n",
    "ax[1].set_title(\"Feature Importance Decision Tree\")\n",
    "ax[2].set_title(\"Feature Importance Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest optimieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle_randomforest_baysian = \"data_science/randomforest_baysian.pkl\"\n",
    "\n",
    "if os.path.exists(pickle_randomforest_baysian):\n",
    "    print(\"Lade bereits gespeicherte RandomForestBaysian Ergebnisse...\")\n",
    "    with open(pickle_randomforest_baysian, \"rb\") as file:\n",
    "        study = pickle.load(file)\n",
    "else:\n",
    "    print(\"Erstelle Random-Forest-Baysian Optimierung...\")\n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Optimization of the RandomForest Hyperparameter\"\"\"\n",
    "\n",
    "        #Searchspace\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", 50, 250)\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 3, 15)\n",
    "        max_features = trial.suggest_categorical(\"max_features\", [\"log2\", \"sqrt\"])\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10, step=2)\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 4)\n",
    "\n",
    "        #Model\n",
    "        params = {\n",
    "            \"n_estimators\": n_estimators,\n",
    "            \"max_features\": max_features,\n",
    "            \"max_depth\": max_depth,\n",
    "            \"min_samples_split\": min_samples_split,\n",
    "            \"min_samples_leaf\": min_samples_leaf\n",
    "        }\n",
    "\n",
    "        model_rf = RandomForestClassifier(class_weight=\"balanced\", random_state=42, **params)\n",
    "\n",
    "        num_cols = features_train.select_dtypes(include=[\"int64\"]).columns\n",
    "        cat_cols = features_train.select_dtypes(include=[\"object\"]).columns \n",
    "\n",
    "        preprocessor = ColumnTransformer([\n",
    "            (\"num\", StandardScaler(), num_cols),\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "        ])\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", model_rf)\n",
    "        ])\n",
    "\n",
    "        score = cross_val_score(\n",
    "            estimator=pipeline, \n",
    "            X=features_train, \n",
    "            y=target_train, \n",
    "            scoring=\"balanced_accuracy\",\n",
    "            cv=3,\n",
    "            n_jobs=1\n",
    "        ).mean()\n",
    "\n",
    "        return score\n",
    "\n",
    "    # create a study and setting a seed for reproduceability\n",
    "    study = optuna.create_study(sampler=TPESampler(seed=42), direction='maximize')\n",
    "\n",
    "    # perform hyperparameter tuning\n",
    "    time_start = time.time()\n",
    "    study.optimize(objective, n_trials=30)\n",
    "    time_bayesian = time.time() - time_start\n",
    "\n",
    "    # store result in a data frame \n",
    "    values_bayesian = [\n",
    "        30, \n",
    "        study.best_trial.number, \n",
    "        study.best_trial.value, \n",
    "        time_bayesian\n",
    "    ]\n",
    "\n",
    "    results_bayesian = pd.DataFrame([values_bayesian], columns=[\n",
    "        \"Number of iterations\", \n",
    "        \"Iteration Number of Optimal Hyperparameters\", \n",
    "        \"Score\", \n",
    "        \"Time Elapsed (s)\"\n",
    "    ])\n",
    "\n",
    "    # best hyperparameter\n",
    "    print(\"\\nBeste Hyperparameter für Random Forest:\")\n",
    "    print(study.best_trial.params)\n",
    "\n",
    "    # show results\n",
    "    print(\"\\nOptimierungsergebnisse:\")\n",
    "    print(results_bayesian)\n",
    "\n",
    "    # Daten speichern\n",
    "    with open(pickle_randomforest_baysian, \"wb\") as file:\n",
    "        pickle.dump(study, file)\n",
    "\n",
    "    print(\"Daten wurden gespeichert!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung der Hyperparameter\n",
    "\n",
    "best_params = study.best_params\n",
    "\n",
    "model_rf_ba = RandomForestClassifier(class_weight=\"balanced\", random_state=42, **best_params)\n",
    "pipeline_rf_ba = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", model_rf_ba)\n",
    "])\n",
    "\n",
    "target_pred_rf_ba = train_and_predict(pipeline_rf_ba, features_train, target_train, features_test, target_test)\n",
    "metrics_rf_ba = evaluate_model(target_test, target_pred_rf_ba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ergebnisse der drei Basismodelle\n",
    "model_results = {\n",
    "    \"Logistic Regression\": metrics_log,\n",
    "    \"Decision Tree\": metrics_dt,\n",
    "    \"Random Forest\": metrics_rf,\n",
    "    \"Baysian RF\": metrics_rf_ba\n",
    "}\n",
    "\n",
    "# Extrahieren der Metriken\n",
    "metrics = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC-AUC\"]\n",
    "model_names = list(model_results.keys())\n",
    "\n",
    "bar_width = 0.2\n",
    "x = np.arange(len(metrics))\n",
    "color_model= [\"silver\", \"teal\", \"midnightblue\", \"rebeccapurple\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for i, (model, color) in enumerate(zip(model_names, color_model)):\n",
    "    values = [model_results[model][metric] for metric in metrics]\n",
    "    ax.bar(x + i * bar_width, values, width=bar_width, label=model, color=color)\n",
    "\n",
    "ax.set_xticks(x + bar_width)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Modellvergleich der Metriken\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness-Metriken\n",
    "\n",
    "ROC-AUC nach Gruppen (1)\n",
    "Diese Metrik zeigt, wie gut ein Modell zwischen verschiedenen Klassen für verschiedene demografische Gruppen unterscheiden kann, wobei Unterschiede auf mögliche Fairnessprobleme hindeuten können.\n",
    "\n",
    "EOD (Equal Opportunity Difference) (0)\n",
    "Misst die Differenz zwischen den True-Positive-Raten verschiedener demografischer Gruppen und zeigt an, ob ein Modell gleiche Chancen für positive Vorhersagen bei tatsächlich positiven Fällen bietet.\n",
    "\n",
    "DPR (Disparate Positive Rate)(1)\n",
    "Vergleicht die Verhältnisse positiver Vorhersagen zwischen verschiedenen demografischen Gruppen und quantifiziert, ob das Modell für bestimmte Gruppen häufiger positive Ergebnisse vorhersagt.\n",
    "\n",
    "DPD (Demographic Parity Difference)(0) Misst den absoluten Unterschied zwischen den Raten positiver Vorhersagen verschiedener demografischer Gruppen und zeigt, ob das Modell statistische Parität unabhängig von der tatsächlichen Verteilung erreicht.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metric_log = evaluate_discrimination(target_test, target_pred_log, features_test)\n",
    "fair_metric_dt = evaluate_discrimination(target_test, target_pred_dt, features_test)\n",
    "fair_metric_rf = evaluate_discrimination(target_test, target_pred_rf, features_test)\n",
    "fair_metric_rf_ba = evaluate_discrimination(target_test, target_pred_rf_ba, features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\"Logistische Regression\", \"Decision Tree\", \"Random Forest\", \"Baysian RF\"]\n",
    "metrics = [fair_metric_log, fair_metric_dt, fair_metric_rf, fair_metric_rf_ba]\n",
    "# Erstellen eines übersichtlichen DataFrames\n",
    "results = pd.DataFrame()\n",
    "results['Modell'] = model_names\n",
    "\n",
    "# ROC-AUC Werte hinzufügen\n",
    "for group in ['male', 'female', 'white', 'non_white']:\n",
    "    results[f'ROC-AUC ({group})'] = [round(float(model['roc_auc'][group]), 3) for model in metrics]\n",
    "\n",
    "# Andere Metriken hinzufügen\n",
    "for metric, name in zip(['eod', 'dpr', 'dpd'], ['EOD', 'DPR', 'DPD']):\n",
    "    for group in ['sex', 'race']:\n",
    "        results[f'{name} ({group})'] = [round(float(model[metric][group]), 3) for model in metrics]\n",
    "\n",
    "# Anzeigen des DataFrames\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dictionary mit allen Modellen erstellen\n",
    "models = {\n",
    "    'Logistic Regression': fair_metric_log,\n",
    "    'Decision Tree': fair_metric_dt,\n",
    "    'Random Forest': fair_metric_rf,\n",
    "    'Baysian RF': fair_metric_rf_ba\n",
    "}\n",
    "\n",
    "# Figure mit 2x2 Subplots erstellen\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Farbpalette für die Modelle\n",
    "colors = [\"silver\", \"teal\", \"midnightblue\", \"rebeccapurple\"]\n",
    "\n",
    "# 1. ROC-AUC Vergleich (oben links)\n",
    "ax = axs[0, 0]\n",
    "roc_metrics = [\"male\", \"female\", \"white\", \"non_white\"]\n",
    "x = np.arange(len(roc_metrics))\n",
    "width = 0.2 \n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"roc_auc\"][metric] for metric in roc_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('ROC-AUC Score')\n",
    "ax.set_title('ROC-AUC nach Gruppen und Modellen')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(roc_metrics)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 2. Equalized Odds Difference (oben rechts)\n",
    "ax = axs[0, 1]\n",
    "eod_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(eod_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"eod\"][metric] for metric in eod_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('EOD Wert')\n",
    "ax.set_title('Equalized Odds Difference')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(eod_metrics)\n",
    "ax.axhline(y=0, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 3. Demographic Parity Ratio (unten links)\n",
    "ax = axs[1, 0]\n",
    "dpr_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(dpr_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"dpr\"][metric] for metric in dpr_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('DPR Wert')\n",
    "ax.set_title('Demographic Parity Ratio')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(dpr_metrics)\n",
    "ax.axhline(y=1, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 4. Demographic Parity Difference (unten rechts)\n",
    "ax = axs[1, 1]\n",
    "dpd_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(dpd_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"dpd\"][metric] for metric in dpd_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('DPD Wert')\n",
    "ax.set_title('Demographic Parity Difference')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(dpd_metrics)\n",
    "ax.axhline(y=0, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Fairness-Metriken Vergleich zwischen Modellen', fontsize=16, y=1.02)\n",
    "plt.savefig(r'data_science\\fairness_model_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bewertung dieser Modelle\n",
    "\n",
    "Fairness-Beurteilung:\n",
    "- Random Forest hat die höchste Disparitäten bei EOD und hohe DPR-Werte\n",
    "- Logistische Regression zeigt besser Metriken\n",
    "- Baysian RF bietet beste Fairness insgesamt\n",
    "\n",
    "Präzision\n",
    "- bisher RandomForest\n",
    "\n",
    "Bester Mittelweg:\n",
    "- Randomforest\n",
    "    Bietet den besten Kompromiss zwischen Leistung und Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fairness Constraint\n",
    "- In-Processing\n",
    "- Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FairnessConstraints\n",
    "constraint = DemographicParity()  # Alternativ: EqualizedOdds() -> würde noch länger laufen\n",
    "fair_model_rf = ExponentiatedGradient(RandomForestClassifier(n_estimators=100, random_state=42), constraints=constraint)\n",
    "\n",
    "# Preprocessor\n",
    "X_train_processed = preprocessor.fit_transform(features_train)\n",
    "X_train_processed = X_train_processed.toarray()\n",
    "\n",
    "X_test_processed = preprocessor.transform(features_test)\n",
    "X_test_processed = X_test_processed.toarray()\n",
    "\n",
    "# Trainiere das Modell\n",
    "fair_model_rf.fit(X_train_processed, target_train, sensitive_features=features_train[[\"sex\", \"race\"]])\n",
    "\n",
    "# Vorhersagen\n",
    "target_pred_fair_rf = fair_model_rf.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, ob Postprocessing weiter die Fairness beeinflusst\n",
    "\n",
    "# Threshold Optimizer mit Demographic Parity\n",
    "postprocess_model = ThresholdOptimizer(\n",
    "    estimator=fair_model_rf,  \n",
    "    constraints=\"demographic_parity\",  # Alternativ: \"equalized_odds\"\n",
    "    prefit=True  # Modell ist schon trainiert\n",
    ")\n",
    "\n",
    "# Trainiere den Postprocessor\n",
    "postprocess_model.fit(X_train_processed, target_train, sensitive_features=features_train[[\"sex\", \"race\"]])\n",
    "\n",
    "# Berechne faire Vorhersagen\n",
    "target_pred_fair_rf_post = postprocess_model.predict(X_test_processed, sensitive_features=features_test[[\"sex\", \"race\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_metric_rf_fc = evaluate_discrimination(target_test, target_pred_fair_rf, features_test)\n",
    "fair_metric_rf_fc_post = evaluate_discrimination(target_test, target_pred_fair_rf_post, features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabelle der Daten\n",
    "model_names_opt = [\"Logistische Regression\", \"Decision Tree\", \"Random Forest\", \"Baysian RF\", \"optim. RF\", \"opt. RF inkl. post-processing\"]\n",
    "metrics_opt = [fair_metric_log, fair_metric_dt, fair_metric_rf, fair_metric_rf_ba, fair_metric_rf_fc,fair_metric_rf_fc_post ]\n",
    "# Erstellen eines übersichtlichen DataFrames\n",
    "results_opt = pd.DataFrame()\n",
    "results_opt['Modell'] = model_names_opt\n",
    "\n",
    "# ROC-AUC Werte hinzufügen\n",
    "for group in ['male', 'female', 'white', 'non_white']:\n",
    "    results_opt[f'ROC-AUC ({group})'] = [round(float(model['roc_auc'][group]),3) for model in metrics_opt]\n",
    "\n",
    "# Andere Metriken hinzufügen\n",
    "for metric, name in zip(['eod', 'dpr', 'dpd'], ['EOD', 'DPR', 'DPD']):\n",
    "    for group in ['sex', 'race']:\n",
    "        results_opt[f'{name} ({group})'] = [round(float(model[metric][group]), 3) for model in metrics_opt]\n",
    "\n",
    "# Anzeigen des DataFrames\n",
    "display(results_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mit allen Modellen erstellen\n",
    "models = {\n",
    "#    'Logistic Regression': fair_metric_log,\n",
    "#    'Decision Tree': fair_metric_dt,\n",
    "    'Random Forest': fair_metric_rf,\n",
    "#    'Baysian RF': fair_metric_rf_ba,\n",
    "    'Optimierter RF': fair_metric_rf_fc,\n",
    "    \"Optimierter RF incl. post-processing\": fair_metric_rf_fc_post \n",
    "}\n",
    "\n",
    "# Figure mit 2x2 Subplots erstellen\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Farbpalette für die Modelle\n",
    "colors = [\n",
    "#    \"silver\", \n",
    "#          \"teal\", \n",
    "          \"midnightblue\", \n",
    "#          \"rebeccapurple\", \n",
    "          \"darkslategray\", \n",
    "          \"mediumorchid\"]\n",
    "\n",
    "# 1. ROC-AUC Vergleich (oben links)\n",
    "ax = axes[0, 0]\n",
    "roc_metrics = [\"male\", \"female\", \"white\", \"non_white\"]\n",
    "x = np.arange(len(roc_metrics))\n",
    "width = 0.2 \n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"roc_auc\"][metric] for metric in roc_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('ROC-AUC Score')\n",
    "ax.set_title('ROC-AUC nach Gruppen und Modellen (Optimum 1)')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(roc_metrics)\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 2. Equalized Odds Difference (oben rechts)\n",
    "ax = axes[0, 1]\n",
    "eod_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(eod_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"eod\"][metric] for metric in eod_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('EOD Wert')\n",
    "ax.set_title('Equalized Odds Difference (Optimum 0)')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(eod_metrics)\n",
    "ax.axhline(y=0, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 3. Demographic Parity Ratio (unten links)\n",
    "ax = axes[1, 0]\n",
    "dpr_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(dpr_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"dpr\"][metric] for metric in dpr_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('DPR Wert')\n",
    "ax.set_title('Demographic Parity Ratio (Optimum 1)')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(dpr_metrics)\n",
    "ax.axhline(y=1, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "# 4. Demographic Parity Difference (unten rechts)\n",
    "ax = axes[1, 1]\n",
    "dpd_metrics = [\"sex\", \"race\"]\n",
    "x = np.arange(len(dpd_metrics))\n",
    "\n",
    "for i, (model_name, fairness_metrics) in enumerate(models.items()):\n",
    "    values = [fairness_metrics[\"dpd\"][metric] for metric in dpd_metrics]\n",
    "    ax.bar(x + i*width, values, width, label=model_name, color=colors[i])\n",
    "\n",
    "ax.set_ylabel('DPD Wert')\n",
    "ax.set_title('Demographic Parity Difference (Optimum 0)')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(dpd_metrics)\n",
    "ax.axhline(y=0, color='g', linestyle='--', alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Fairness-Metriken Vergleich zwischen Modellen', fontsize=16, y=1.02)\n",
    "plt.savefig(r'data_science\\fairness_model_comparison_opt.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bewertung\n",
    "Der optimierte Random Forest zeigt deutliche Verbesserungen in den Fairness-Metriken\n",
    "Aber: hat Einbuße bei den ROC-AUC-Werten\n",
    "\n",
    "Extreme Verbesserung bei DPR und DPD. wurde aber auch dahingehend optimiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToDos\n",
    "- Trade-off zwischen Fairness und Performance: Wie stark werden die ROC-AUC-Werte durch die Fairness-Optimierung beeinträchtigt. Bei dem optimierten RF sind leichte Einbußen bei der Performance zu erkennen.\n",
    "\n",
    "- Unterschiede zwischen Fairness-Metriken: Die Optimierung verbessert DPR (sex), aber EOD weniger stark. Eventuell 2. Analyse nach EOD ausgerichtet\n",
    "\n",
    "- Post-Processing vs. In-Training-Optimierung: Vergleich, wie sich das Post-Processing auf die einzelnen Fairness-Metriken im Vergleich zur reinen ExponentiatedGradient-Optimierung auswirkt.\n",
    "\n",
    "- Weiteres Feature-Engineering\n",
    "- Random-Forest Baysian auch auf andere scorings ansetzen und danach In-Training-Optimierung und Post-Processing anwenden\n",
    "\n",
    "- Theoretisch: Verallgemeinerbarkeit, lässt sich das auf andere Datensetze ebenso projizieren\n",
    "\n",
    "\n",
    "- LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
